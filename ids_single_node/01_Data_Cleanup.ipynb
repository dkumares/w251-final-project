{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fd0f3d3",
   "metadata": {},
   "source": [
    "## DATA CLEANUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28374b4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T14:44:29.536101Z",
     "start_time": "2021-04-04T14:44:29.526135Z"
    }
   },
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "# Notebook to be used for cleaning up the raw data files,  special values, etc.    # \n",
    "####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e6f7ab2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T14:44:29.555208Z",
     "start_time": "2021-04-04T14:44:29.543759Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "903e748e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T14:44:29.577949Z",
     "start_time": "2021-04-04T14:44:29.562816Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a set of the raw data and processed files name\n",
    "# CONFIG NEEDED: Uncomment only the specific files to be processed on your node\n",
    "\n",
    "csv_files = {\n",
    " '02-14-2018.csv': '02-14-2018-bruteforce-ftp-ssh.csv',\n",
    " '02-15-2018.csv': '02-15-2018-dos-goldeneye-slowloris.csv',\n",
    " '02-16-2018.csv': '02-16-2018-dos-slowhttp-hulk.csv',\n",
    " '02-21-2018.csv': '02-21-2018-ddos-loic-udp.csv',\n",
    " '02-22-2018.csv': '02-22-2018-bruteforce-webxss.csv',\n",
    " '02-23-2018.csv': '02-23-2018-bruteforce-webxss-sql.csv',\n",
    " '02-28-2018.csv': '02-28-2018-infiltration.csv',\n",
    " '03-01-2018.csv': '03-01-2018-botnet.csv',\n",
    " '03-02-2018.csv': '03-02-2018-infiltration.csv',\n",
    " '02-20-2018.csv': '02-20-2018-ddos-loic-tcp.csv'   # WARNING: 4GB FILE.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e6908d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T14:44:29.598740Z",
     "start_time": "2021-04-04T14:44:29.585206Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the folder name for raw data and processed files under the project directory\n",
    "# CONFIG NEEDED: Change the './data' and 'processed' to what you named your directories\n",
    "# Raw Data Files Location: final_project/data\n",
    "# Processed Data Files Location: final_project/data/processed\n",
    "\n",
    "rawdata_path = '../data'\n",
    "processed_path = os.path.join(rawdata_path, 'processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "192e80a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T14:44:29.621159Z",
     "start_time": "2021-04-04T14:44:29.608175Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove duplicate headers\n",
    "def remove_headers(f):    \n",
    "    return f[~f['Dst Port'].str.contains('Dst Port', na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80c78ce1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T14:44:29.664955Z",
     "start_time": "2021-04-04T14:44:29.650187Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop rows that have 'Infinity', 'infinity', or 'inf' as value\n",
    "def drop_infinity(f):\n",
    "    # Remove infinity and NaN values    \n",
    "    num_of_raw_records = f.shape[0]\n",
    "    \n",
    "    print('Number of Infinity or NaN Values')\n",
    "    print(f.isin([np.nan, np.inf, -np.inf]).sum().sum())\n",
    "\n",
    "    # Replace infinity to NaN and drop NaN values\n",
    "    f = f.replace([np.inf, -np.inf], np.nan)\n",
    "    f = f.dropna()\n",
    "    f = f.reset_index(drop=True)\n",
    "\n",
    "    dropped_NaN_records = num_of_raw_records - f.shape[0]\n",
    "    print('Number of NaN/Inf Records Dropped: ', dropped_NaN_records)\n",
    "\n",
    "    # Check infinity and NaN values\n",
    "    print('Remaining Infinity or NaN Values')\n",
    "    print(f.isin([np.nan, np.inf, -np.inf]).sum().sum())\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7864a472",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T14:44:29.713422Z",
     "start_time": "2021-04-04T14:44:29.698456Z"
    }
   },
   "outputs": [],
   "source": [
    "column_name_regex = re.compile(r\"\\W\", re.IGNORECASE)\n",
    "\n",
    "# Clean (spaces, special characters, etc.) column headers and lower case \n",
    "def remove_non_word_chars_from_column_names(f):\n",
    "    return [column_name_regex.sub('_', c.lower()) for c in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87b02d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T14:45:15.215724Z",
     "start_time": "2021-04-04T14:44:29.744044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading: ../data/02-14-2018.csv\n",
      "processing: ../data/02-14-2018.csv\n",
      "Number of Infinity or NaN Values\n",
      "2277\n",
      "Number of NaN/Inf Records Dropped:  2277\n",
      "Remaining Infinity or NaN Values\n",
      "0\n",
      "writing: ../data/processed/02-14-2018-bruteforce-ftp-ssh.csv\n",
      "reading: ../data/02-15-2018.csv\n",
      "processing: ../data/02-15-2018.csv\n",
      "Number of Infinity or NaN Values\n",
      "4921\n",
      "Number of NaN/Inf Records Dropped:  4921\n",
      "Remaining Infinity or NaN Values\n",
      "0\n",
      "writing: ../data/processed/02-15-2018-dos-goldeneye-slowloris.csv\n",
      "reading: ../data/02-16-2018.csv\n",
      "processing: ../data/02-16-2018.csv\n",
      "Number of Infinity or NaN Values\n",
      "0\n",
      "Number of NaN/Inf Records Dropped:  0\n",
      "Remaining Infinity or NaN Values\n",
      "0\n",
      "writing: ../data/processed/02-16-2018-dos-slowhttp-hulk.csv\n",
      "reading: ../data/02-21-2018.csv\n",
      "processing: ../data/02-21-2018.csv\n",
      "Number of Infinity or NaN Values\n",
      "0\n",
      "Number of NaN/Inf Records Dropped:  0\n",
      "Remaining Infinity or NaN Values\n",
      "0\n",
      "writing: ../data/processed/02-21-2018-ddos-loic-udp.csv\n",
      "reading: ../data/02-22-2018.csv\n",
      "processing: ../data/02-22-2018.csv\n",
      "Number of Infinity or NaN Values\n",
      "3569\n",
      "Number of NaN/Inf Records Dropped:  3569\n",
      "Remaining Infinity or NaN Values\n",
      "0\n",
      "writing: ../data/processed/02-22-2018-bruteforce-webxss.csv\n",
      "reading: ../data/02-23-2018.csv\n"
     ]
    }
   ],
   "source": [
    "# Create folder for processed files if none exists\n",
    "if not os.path.exists(processed_path):\n",
    "    os.mkdir(processed_path)    \n",
    "\n",
    "# Process the list of files specified\n",
    "for f, out in csv_files.items():\n",
    "    file_path = os.path.join(rawdata_path, f)\n",
    "    output_path = os.path.join(processed_path, out)\n",
    "\n",
    "    print('reading:', file_path)\n",
    "    # One of the data files has four extra columns which need to be dropped. Checks each file for extra columns.\n",
    "    df = pd.read_csv(file_path, dtype=str).drop(columns=['Flow ID', 'Src IP', 'Dst IP', 'Src Port'], errors='ignore')\n",
    "\n",
    "    # Clean up the data files\n",
    "    print('processing:', file_path)\n",
    "    df = remove_headers(df)\n",
    "    df.columns = remove_non_word_chars_from_column_names(df)\n",
    "    df = drop_infinity(df)\n",
    "\n",
    "    # Write it as a cleaned file in the processed directory\n",
    "    print('writing:', output_path)\n",
    "    df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6334467",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T14:45:15.232964Z",
     "start_time": "2021-04-04T14:44:29.357Z"
    }
   },
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# Cells below this are only needed if you want to test if the files were created correctly #\n",
    "# Comment/Uncomment as needed\n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902fee8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T14:45:15.238874Z",
     "start_time": "2021-04-04T14:44:29.360Z"
    }
   },
   "outputs": [],
   "source": [
    "print(os.listdir(processed_path))  # Print list of files in the processed directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1da94f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T14:45:15.244199Z",
     "start_time": "2021-04-04T14:44:29.363Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read a sample file and check label counts\n",
    "# CONFIG NEEDED: Change file name to the file you want to check\n",
    "df = pd.read_csv(\"../data/processed/03-02-2018-infiltration.csv\")\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d445f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
