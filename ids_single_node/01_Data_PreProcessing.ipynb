{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dd7f771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3f76f686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of the raw data and processed files name\n",
    "# CONFIG NEEDED: Uncomment only the specific files to be processed on your node\n",
    "\n",
    "csv_files = [\n",
    " '02-14-2018.csv',\n",
    " '02-15-2018.csv',\n",
    " '02-16-2018.csv',\n",
    " '02-21-2018.csv',\n",
    " '02-22-2018.csv',\n",
    " '02-23-2018.csv',\n",
    " '02-28-2018.csv',\n",
    " '03-01-2018.csv',\n",
    " '03-02-2018.csv'\n",
    "]\n",
    "\n",
    "label_maps = {'Benign': 0, 'FTP-BruteForce': 1, 'SSH-Bruteforce': 1, 'DoS attacks-GoldenEye': 1, 'DoS attacks-Slowloris': 1,\n",
    "         'DoS attacks-SlowHTTPTest': 1, 'DoS attacks-Hulk': 1, 'Brute Force -Web': 1, 'Brute Force -XSS': 1,\n",
    "         'SQL Injection': 1, 'Infilteration': 1, 'Bot': 1, 'DDOS attack-HOIC': 1, 'DDoS attacks-LOIC-HTTP': 1, \n",
    "         'DDOS attack-LOIC-UDP': 1}\n",
    " \n",
    "# CONFIG NEEDED: Change Binary and Multi-class output file names if needed\n",
    "multi_class_file = 'DATA-IDS-2018-multiclass'\n",
    "binary_class_file = 'DATA-IDS-2018-binaryclass'\n",
    "\n",
    "# CONFIG NEEDED: Change Train and Test output file names if needed. Adjust the split size.\n",
    "test_prefix = 'TEST-'\n",
    "train_prefix = 'TRAIN-'\n",
    "\n",
    "test_size = 0.10\n",
    "num_trainers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "83b94548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the folder name for raw data and processed files under the project directory\n",
    "# CONFIG NEEDED: Change the './data' and 'processed' to what you named your directories\n",
    "# Raw Data Files Location: final_project/data\n",
    "# Processed Data Files Location: final_project/data/processed\n",
    "\n",
    "rawdata_path = '../data'\n",
    "processed_path = os.path.join(rawdata_path, 'processed')\n",
    "\n",
    "# CONFIG NEEDED: Change to true as needed for multi-class or binary class files. \n",
    "# Note atleast one of these has to be true for the combined data file to be created. \n",
    "multi_class = True\n",
    "binary_class = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899e136e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading: ../data/02-14-2018.csv\n",
      "appending: ../data/02-15-2018.csv\n",
      "appending: ../data/02-16-2018.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appending: ../data/02-21-2018.csv\n",
      "appending: ../data/02-22-2018.csv\n",
      "appending: ../data/02-23-2018.csv\n",
      "appending: ../data/02-28-2018.csv\n"
     ]
    }
   ],
   "source": [
    "# Read the first file from the list to be processed\n",
    "fname = os.path.join(rawdata_path, csv_files[0])\n",
    "print('reading:', fname)\n",
    "df = pd.read_csv(fname).drop(columns=['Flow ID', 'Src IP', 'Dst IP', 'Src Port'], errors='ignore')\n",
    "\n",
    "# Read the remaining files in the list\n",
    "for name in csv_files[1:]:\n",
    "    fname = os.path.join(rawdata_path, name)\n",
    "    print('appending:', fname)\n",
    "    df1 = pd.read_csv(fname).drop(columns=['Flow ID', 'Src IP', 'Dst IP', 'Src Port'], errors='ignore')\n",
    "    df = df.append(df1, ignore_index=True)\n",
    "\n",
    "# Shuffle the data records and print final shape\n",
    "print('Combined Raw Datafile Shape')\n",
    "print(df.shape)\n",
    "\n",
    "num_of_raw_records = df.shape[0]\n",
    "print('Original Number of Records: ', num_of_raw_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92315db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove infinity and NaN values\n",
    "print('Number of Infinity or NaN Values')\n",
    "print(df.isin([np.nan, np.inf, -np.inf]).sum().sum())\n",
    "\n",
    "# Replace infinity to NaN and drop NaN values\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "dropped_NaN_records = num_of_raw_records - df.shape[0]\n",
    "print('Number of NaN/Inf Records Dropped: ', dropped_NaN_records)\n",
    "\n",
    "# Check infinity and NaN values\n",
    "print('Remaining Infinity or NaN Values')\n",
    "print(df.isin([np.nan, np.inf, -np.inf]).sum().sum())\n",
    "\n",
    "print('Combined Raw Datafile Shape')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc3f981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate headers\n",
    "df = df[~df['Dst Port'].str.contains('Dst Port', na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f145c2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean (spaces, special characters, etc.) column headers and lower case \n",
    "column_name_regex = re.compile(r\"\\W\", re.IGNORECASE)\n",
    "\n",
    "df.columns = [column_name_regex.sub('_', c.lower()) for c in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe582d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original Dataset Value Counts')\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deddf939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop attack types that have less than 20K rows.\n",
    "df = df.groupby('label').filter(lambda x : len(x) > 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77273c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataset Value Counts After Dropping Minimal Attacks')\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b29ad4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into test and train data\n",
    "y = df.pop('label')\n",
    "X = df\n",
    "\n",
    "# split into train test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y, shuffle=True, random_state=1)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "dftrain = X_train.join(y_train)\n",
    "dftest = X_test.join(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205ba830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multi-class label file\n",
    "if multi_class:\n",
    "    print('Creating Multi-Class Test File')\n",
    "    outTestFile = os.path.join(processed_path, test_prefix + '-' + multi_class_file + '.csv')\n",
    "    dftest = dftest.drop('timestamp', axis=1)      # Drop timestamp column\n",
    "    dftest.to_csv(outTestFile, index=False)\n",
    "    print('finished writing:', outTestFile)\n",
    "    \n",
    "    # Sort training data based of timestamp and split into four equal chunks\n",
    "    dftrain = dftrain.sort_values(by='timestamp', ascending=True)\n",
    "    df_train_split = np.array_split(dftrain, num_trainers)\n",
    "    \n",
    "    for x in range(0, num_trainers):\n",
    "        print('Creating Multi-Class Training File: ', str(x+1))\n",
    "        outTrainFile = os.path.join(processed_path, train_prefix + str(x+1) + '-' + multi_class_file + '.csv')\n",
    "        df_train_split[x].to_csv(outTrainFile, index=False)\n",
    "        print('finished writing:', outTrainFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea39c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/processed/TRAIN-0DATA-IDS-2018-multiclass.csv\")\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a334c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if binary_class:\n",
    "#     print('Creating Binary-Class Test File')\n",
    "#     df = pd.read_csv(processed_path, test_prefix + multi_class_file + '.csv')\n",
    "#     outTestFile = os.path.join(processed_path, test_prefix + binary_class_file + '.csv')\n",
    "\n",
    "#     # Map benign rows to 0, all others as 1\n",
    "#     df['label'] = df['label'].map(label_maps).astype(int)\n",
    "#     df.to_csv(outTestFile, index=False)\n",
    "#     print('finished writing:', outTestFile)\n",
    "\n",
    "#     for x in range(0, num_trainers):\n",
    "#         print('Creating Binary-Class Training File: ', str(x+1))\n",
    "#         df = pd.read_csv(processed_path, train_prefix + str(x+1) + multi_class_file + '.csv')\n",
    "\n",
    "#         # Map benign rows to 0, all others as 1\n",
    "#         df['label'] = df['label'].map(label_maps).astype(int)\n",
    "\n",
    "#         outTrainFile = os.path.join(processed_path, train_prefix + str(x+1) + binary_class_file + '.csv')\n",
    "#         df.to_csv(outTrainFile, index=False)\n",
    "#         print('finished writing:', outTrainFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f505679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"../data/processed/train-02-20-2018-ddos-loic-tcp.csv\")\n",
    "# df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e773eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491dbfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('all done...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
