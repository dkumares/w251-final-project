{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDS_df = pd.read_csv(\"data/ids_small.csv\")\n",
    "\n",
    "# To display the top 5 rows\n",
    "# IDS_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 80)\n"
     ]
    }
   ],
   "source": [
    "# print shape before dropping NaN rows\n",
    "print(IDS_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n"
     ]
    }
   ],
   "source": [
    "# Finding the null values.\n",
    "print(IDS_df.isin([np.nan, np.inf, -np.inf]).sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  first replace infs to NaN:\n",
    "IDS_df = IDS_df.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17941, 80)\n"
     ]
    }
   ],
   "source": [
    "# print shape after dropping NaN rows\n",
    "IDS_df = IDS_df.dropna()\n",
    "print(IDS_df.shape)\n",
    "IDS_df = IDS_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Finding the null values.\n",
    "print(IDS_df.isin([np.nan, np.inf, -np.inf]).sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the proportion of types of traffic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Benign                      7963\n",
       "Infilteration               1978\n",
       "DoS attacks-Hulk            1000\n",
       "Bot                         1000\n",
       "DoS attacks-Slowloris       1000\n",
       "SSH-Bruteforce              1000\n",
       "FTP-BruteForce              1000\n",
       "DoS attacks-GoldenEye       1000\n",
       "DDOS attack-HOIC            1000\n",
       "DoS attacks-SlowHTTPTest    1000\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IDS_df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert all non-normal observations into a single class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(text):\n",
    "    \"\"\"Binarize target labels into normal or anomalous.\"\"\"\n",
    "    if text == \"Benign\":\n",
    "        return 0\n",
    "    elif text == 'Infilteration':\n",
    "        return 1\n",
    "    elif text == 'DoS attacks-Slowloris':\n",
    "        return 2\n",
    "    elif text == 'SSH-Bruteforce':\n",
    "        return 3\n",
    "    elif text == 'DDOS attack-HOIC':\n",
    "        return 4\n",
    "    elif text == 'FTP-BruteForce':\n",
    "        return 5\n",
    "    elif text == 'DoS attacks-SlowHTTPTest':\n",
    "        return 6\n",
    "    elif text == 'Bot':\n",
    "        return 7\n",
    "    elif text == 'DoS attacks-Hulk':\n",
    "        return 8\n",
    "    elif text == 'DoS attacks-GoldenEye':\n",
    "        return 9\n",
    "\n",
    "IDS_df[\"Label\"] = IDS_df[\"Label\"].apply(get_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    7963\n",
      "1    1978\n",
      "9    1000\n",
      "8    1000\n",
      "7    1000\n",
      "6    1000\n",
      "5    1000\n",
      "4    1000\n",
      "3    1000\n",
      "2    1000\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y = IDS_df[\"Label\"].values\n",
    "print(IDS_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert all categorical features into numerical form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encodings_dictionary = dict()\n",
    "for c in IDS_df.columns:\n",
    "    if IDS_df[c].dtype == \"object\":\n",
    "        encodings_dictionary[c] = LabelEncoder()\n",
    "        IDS_df[c] = encodings_dictionary[c].fit_transform(IDS_df[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into normal and abnormal observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDS_df_normal = IDS_df[IDS_df[\"Label\"] == 0].sample(1000)  # Taking only 1000 samples to balance. Change this as required.\n",
    "IDS_df_abnormal = IDS_df[IDS_df[\"Label\"] != 0]\n",
    "y_normal = IDS_df_normal.pop(\"Label\").values\n",
    "X_normal = IDS_df_normal.values\n",
    "y_anomaly = IDS_df_abnormal.pop(\"Label\").values\n",
    "X_anomaly = IDS_df_abnormal.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-test split the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_normal_train, X_normal_test, y_normal_train, y_normal_test = train_test_split(\n",
    "    X_normal, y_normal, test_size=0.2, random_state=11\n",
    ")\n",
    "\n",
    "X_anomaly_train, X_anomaly_test, y_anomaly_train, y_anomaly_test = train_test_split(\n",
    "    X_anomaly, y_anomaly, test_size=0.2, random_state=11\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X_train = np.concatenate((X_normal_train, X_anomaly_train))\n",
    "y_train = np.concatenate((y_normal_train, y_anomaly_train))\n",
    "X_test = np.concatenate((X_normal_test, X_anomaly_test))\n",
    "y_test = np.concatenate((y_normal_test, y_anomaly_test))\n",
    "\n",
    "# X_train = np.concatenate((X_normal_train[:10000], X_anomaly_train[:10000]))\n",
    "# y_train = np.concatenate((y_normal_train[:10000], y_anomaly_train[:10000]))\n",
    "# X_test = np.concatenate((X_normal_test[:1000], X_anomaly_test[:1000]))\n",
    "# y_test = np.concatenate((y_normal_test[:1000], y_anomaly_test[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8782, 79)\n",
      "(2196, 79)\n",
      "(8782,)\n",
      "(2196,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed loading data\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "# Pytorch\n",
    "X_train  = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train)\n",
    "\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test)\n",
    "\n",
    "# Pytorch train and test sets\n",
    "train = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "valid = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = False)\n",
    "valid_loader = torch.utils.data.DataLoader(valid, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "print('Completed loading data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8782, 79])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.tensors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layer1): Linear(in_features=79, out_features=512, bias=True)\n",
      "  (activ1): ReLU()\n",
      "  (layer2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (activ2): ReLU()\n",
      "  (layer3): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (activ3): ReLU()\n",
      "  (layer4): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "        \n",
    "# Defining the DNN model\n",
    "input_size = train_loader.dataset.tensors[0].shape[1]\n",
    "hidden_layers = [512, 512, 512]\n",
    "output_size = 10\n",
    "\n",
    "# model definition\n",
    "class MLP(nn.Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_inputs, hidden_layers[0])\n",
    "        self.activ1 = nn.ReLU()\n",
    "        \n",
    "        self.layer2 = nn.Linear(hidden_layers[0], hidden_layers[1])\n",
    "        self.activ2 = nn.ReLU()\n",
    "        \n",
    "        self.layer3 = nn.Linear(hidden_layers[1], hidden_layers[2])\n",
    "        self.activ3 = nn.ReLU()\n",
    "        \n",
    "        self.layer4 = nn.Linear(hidden_layers[2], output_size)\n",
    "        \n",
    "    # forward propagate input\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.activ1(x)\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        x = self.activ2(x)\n",
    "        \n",
    "        x = self.layer3(x)\n",
    "        x = self.activ3(x)\n",
    "        \n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "model = MLP(input_size)\n",
    "print(model)\n",
    "model.to(device)\n",
    "\n",
    " # Cross Entropy Loss \n",
    "error = nn.CrossEntropyLoss().to(device)\n",
    "# SGD Optimizer\n",
    "learning_rate = 0.001\n",
    "# TODO: Try SGD\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Epoch: 1 Iteration: 0  Loss: 314954.5  Accuracy: 9 %\n",
      "Epoch: 2 Iteration: 0  Loss: 1269992.625  Accuracy: 46 %\n",
      "Epoch: 3 Iteration: 0  Loss: 196460.984375  Accuracy: 41 %\n",
      "Epoch: 4 Iteration: 0  Loss: 2604.7255859375  Accuracy: 33 %\n",
      "Epoch: 5 Iteration: 0  Loss: 2.8290276527404785  Accuracy: 28 %\n",
      "Epoch: 6 Iteration: 0  Loss: 2.5379722118377686  Accuracy: 37 %\n",
      "Epoch: 7 Iteration: 0  Loss: 2.3835365772247314  Accuracy: 37 %\n",
      "Epoch: 8 Iteration: 0  Loss: 2.3223280906677246  Accuracy: 38 %\n",
      "Epoch: 9 Iteration: 0  Loss: 2.270143508911133  Accuracy: 37 %\n",
      "Epoch: 10 Iteration: 0  Loss: 2.2479453086853027  Accuracy: 37 %\n",
      "Epoch: 11 Iteration: 0  Loss: 2.2372217178344727  Accuracy: 37 %\n",
      "Epoch: 12 Iteration: 0  Loss: 2.224053382873535  Accuracy: 38 %\n",
      "Epoch: 13 Iteration: 0  Loss: 2.2303388118743896  Accuracy: 38 %\n",
      "Epoch: 14 Iteration: 0  Loss: 2.2177274227142334  Accuracy: 38 %\n",
      "Epoch: 15 Iteration: 0  Loss: 2.2331197261810303  Accuracy: 38 %\n",
      "Epoch: 16 Iteration: 0  Loss: 2.2059366703033447  Accuracy: 37 %\n",
      "Epoch: 17 Iteration: 0  Loss: 2.209951877593994  Accuracy: 38 %\n",
      "Epoch: 18 Iteration: 0  Loss: 2.2837862968444824  Accuracy: 38 %\n",
      "Epoch: 19 Iteration: 0  Loss: 2.2833995819091797  Accuracy: 38 %\n",
      "Epoch: 20 Iteration: 0  Loss: 2.2534966468811035  Accuracy: 38 %\n",
      "Epoch: 21 Iteration: 0  Loss: 2.2223894596099854  Accuracy: 38 %\n",
      "Epoch: 22 Iteration: 0  Loss: 2.378582239151001  Accuracy: 38 %\n",
      "Epoch: 23 Iteration: 0  Loss: 2.203137159347534  Accuracy: 38 %\n",
      "Epoch: 24 Iteration: 0  Loss: 2.3462862968444824  Accuracy: 38 %\n",
      "Epoch: 25 Iteration: 0  Loss: 2.2346720695495605  Accuracy: 38 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-ee3ff66bae90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Adam does not support sparse gradients, please consider SparseAdam instead'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Start training...')\n",
    "start_time = time.time()\n",
    "    \n",
    "epochs = 500\n",
    "for e in range(epochs):\n",
    "    count = 0\n",
    "    loss_list = []\n",
    "    iteration_list = []\n",
    "    accuracy_list = []\n",
    "   \n",
    "    for i, (data, labels) in enumerate(train_loader):\n",
    "        train = data.to(device)\n",
    "        #print(labels)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward propagation\n",
    "        outputs = model(train)\n",
    "        \n",
    "        # Calculate softmax and cross entropy loss\n",
    "        loss = error(outputs, labels)\n",
    "        # Calculating gradients\n",
    "        loss.backward()\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        if count % 100 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for data, labels in valid_loader:\n",
    "                valid = data.to(device)\n",
    "                #print('Lables:', labels)\n",
    "                \n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward propagation\n",
    "                outputs = model(valid)\n",
    "                # Get predictions from the maximum value\n",
    "                predicted = torch.max(outputs.data, 1)[1]\n",
    "\n",
    "                #print('Predicted: ', predicted)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += len(labels)\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / float(total)\n",
    "\n",
    "            # store loss and iteration\n",
    "            loss_list.append(loss.data)\n",
    "            iteration_list.append(count)\n",
    "            accuracy_list.append(accuracy)\n",
    "        if count % 100 == 0:\n",
    "            # Print Loss\n",
    "            print('Epoch: {} Iteration: {}  Loss: {}  Accuracy: {} %'.format(e + 1, count, loss.data, accuracy))\n",
    "\n",
    "        count += 1\n",
    "\n",
    "end_time = time.time()\n",
    "print('Epochs completed. Time taken (seconds): ', str(end_time - start_time))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
